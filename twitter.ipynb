{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Twitter: sentiments analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\DIARRA née CISSE Bal\\Anaconda3\\lib\\site-packages\\nltk\\twitter\\__init__.py:20: UserWarning: The twython library has not been installed. Some functionality from the twitter package will not be available.\n",
      "  warnings.warn(\"The twython library has not been installed. \"\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import tweepy\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import twitter\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "import datetime as dt\n",
    "import ast\n",
    "from datetime import datetime\n",
    "from datetime import timezone \n",
    "import sqlite3\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tweets scrapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def tweet_search(query, max_tweets,langue):\n",
    "    ''' Function that takes in a search string 'query', the maximum\n",
    "        number of tweets 'max_tweets', and the minimum (i.e., starting)\n",
    "        tweet id. It returns a list of tweepy.models.Status objects. '''\n",
    "    \n",
    "    global liste\n",
    "    api = twitter.Api(consumer_key='GsZD2aeSrKjX1mhpyLwQ3sk6D',\n",
    "  consumer_secret='i6h6mcoKUQ4ReffNRnZWMO2cOvZJg61IugUj2kSjpGmalPdFG6',\n",
    "  access_token_key='969881872124841984-gOZs4DfzAWD2XlxqWsOIam9hestJWlH',\n",
    "  access_token_secret='eTBWSq7ju1Qap896DLQxZDZN2bBkpt1elmnKkCZ7KXnYJ')\n",
    " \n",
    "    searched_tweets = []\n",
    "    while len(searched_tweets) < max_tweets:\n",
    "        remaining_tweets = max_tweets - len(searched_tweets)\n",
    "        #print(\"remaining_tweets:\",remaining_tweets)\n",
    "        try:\n",
    "            new_tweets = api.GetSearch(query, count=100,lang=langue)\n",
    "            #print('found',len(new_tweets),'tweets')\n",
    "            for tweet in new_tweets:\n",
    "                liste.append({\"id\":tweet.id,\"name\":tweet.user.name,\"tweet\": tweet.text,\"created at\": tweet.created_at, \"place\":tweet.place})\n",
    "                #print(\"id:\",tweet.id,\"name:\",tweet.user.name,\"tweet:\",tweet.text,\"langue:\",tweet.lang,\"created at:\", tweet.created_at, \"location:\", tweet.location,\"place:\",tweet.place,\"user_name:\",tweet.user.location,\"user_location:\",tweet.user.location)\n",
    "            if not new_tweets:\n",
    "                print('no tweets found')\n",
    "                break\n",
    "            searched_tweets.extend(new_tweets)\n",
    "        except twitter.error.TwitterError: \n",
    "            print('exception raised, waiting 15 minutes')\n",
    "            print('(until:', dt.datetime.now()+dt.timedelta(minutes=15), ')')\n",
    "            print(\"remaining_tweets:\",remaining_tweets)\n",
    "            time.sleep(15*60)\n",
    "            #break # stop the loop\n",
    "    return liste\n",
    "\n",
    "liste=[]\n",
    "a=tweet_search(query=\"e\",max_tweets=1000,langue=\"en\")\n",
    "df_tweets=pd.DataFrame(a)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tweets cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\DIARRA née CISSE Bal\\Anaconda3\\lib\\site-packages\\pandas\\core\\indexing.py:179: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  self._setitem_with_indexer(indexer, value)\n"
     ]
    }
   ],
   "source": [
    "def clean_tweets(liste_df):\n",
    "    df_tweets=pd.concat(liste_df)\n",
    "    # Drop tweets chere place is unknow\n",
    "    df_tweets=df_tweets.loc[df_tweets.place.notnull()]\n",
    "    # Drop duplicates\n",
    "    df_tweets.drop_duplicates(subset='tweet',inplace=True)\n",
    "    df_tweets.reset_index(drop=True, inplace=True)\n",
    "    dateTime=[]\n",
    "    df_tweets['id']=df_tweets['id'].astype('int64')\n",
    "    # column country and country_code\n",
    "    df_tweets['place']=df_tweets['place'].astype('str')\n",
    "    df_tweets=df_tweets.join(df_tweets['place'].apply(ast.literal_eval).apply(pd.Series)['country'])\n",
    "    df_tweets=df_tweets.join(df_tweets['place'].apply(ast.literal_eval).apply(pd.Series)['country_code'])\n",
    "    \n",
    "    # Date parsing\n",
    "    for x in df_tweets[\"created at\"]:\n",
    "        dateTime.append(datetime.strptime(x, '%a %b %d %H:%M:%S %z %Y').replace(\n",
    "            tzinfo=timezone.utc).astimezone(tz=None).strftime('%Y-%m-%d %H:%M:%S'))\n",
    "    df_tweets['datetime']=dateTime\n",
    "    # from country code of two letters to country code of tree letters\n",
    "    country_code= pd.read_csv(\"csv/country_codes.csv\",encoding = \"ISO-8859-1\",sep=\";\",names=[\"Country\",\"country_code\",\"country_code3\"])  \n",
    "    for i in range(0,len(df_tweets)):\n",
    "        if any(country_code['country_code']==df_tweets['country_code'].loc[i]):\n",
    "            df_tweets['country_code'].loc[i]=country_code[country_code['country_code']==df_tweets['country_code'].loc[i]]['country_code3'].values[0]\n",
    "     #Dropping useless columns\n",
    "    if len(liste_df) ==1:\n",
    "        df_tweets.drop(['created at'], axis=1, inplace=True)\n",
    "    else:\n",
    "        df_tweets.drop(['Unnamed: 0','created at','location','user_name','user_location','langue'], axis=1, inplace=True)\n",
    "    return(df_tweets)\n",
    "\n",
    "df_tweets=clean_tweets([df_tweets])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentiments analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "analyse des sentiments\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\DIARRA née CISSE Bal\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:10: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  # Remove the CWD from sys.path while we load stuff.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "analyse finie\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\DIARRA née CISSE Bal\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:17: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "C:\\Users\\DIARRA née CISSE Bal\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:21: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    }
   ],
   "source": [
    "def tweet_analysis(df_tweets):\n",
    "    \n",
    "    print( \"analyse des sentiments\" )\n",
    "    df_tweets['tweetos'] = '' \n",
    "    df_tweets['text_lem'] = ''\n",
    "\n",
    "    #add tweetos first part\n",
    "    for i in range(len(df_tweets['tweet'])):\n",
    "        try:\n",
    "            df_tweets['tweetos'][i] = df_tweets['tweet'].str.split(' ')[i][0]\n",
    "        except AttributeError:    \n",
    "            df_tweets['tweetos'][i] = 'other'\n",
    "\n",
    "    #Preprocessing tweetos. select tweetos contains 'RT @'\n",
    "    for i in range(len(df_tweets['tweet'])):\n",
    "        if df_tweets['tweetos'].str.contains('@')[i]  == False:\n",
    "            df_tweets['tweetos'][i] = 'other'\n",
    "        \n",
    "    # remove URLs, RTs, and twitter handles\n",
    "    for i in range(len(df_tweets['tweet'])):\n",
    "        df_tweets['text_lem'][i] = \" \".join([word for word in df_tweets['tweet'][i].split()\n",
    "                                    if 'http' not in word and '@' not in word and '<' not in word])\n",
    "    #df_tweets['text_lem'] = [''.join([WordNetLemmatizer().lemmatize(re.sub('[^A-Za-z]', ' ', line)) \n",
    "     #                                 for line in lists]).strip() for lists in df_tweets['tweet']]       \n",
    "    sid = SentimentIntensityAnalyzer()\n",
    "    df_tweets['sentiment_compound_polarity']=df_tweets.text_lem.apply(lambda x:sid.polarity_scores(x)['compound'])\n",
    "    df_tweets['sentiment_neutral']=df_tweets.text_lem.apply(lambda x:sid.polarity_scores(x)['neu'])\n",
    "    df_tweets['sentiment_negative']=df_tweets.text_lem.apply(lambda x:sid.polarity_scores(x)['neg'])\n",
    "    df_tweets['sentiment_pos']=df_tweets.text_lem.apply(lambda x:sid.polarity_scores(x)['pos'])\n",
    "    df_tweets['sentiment_type']=''\n",
    "    df_tweets.loc[df_tweets.sentiment_compound_polarity>0,'sentiment_type']='Positive'\n",
    "    df_tweets.loc[df_tweets.sentiment_compound_polarity==0,'sentiment_type']='Neutral'\n",
    "    df_tweets.loc[df_tweets.sentiment_compound_polarity<0,'sentiment_type']='Negative'\n",
    "    df_tweets.drop(['sentiment_neutral','sentiment_negative','sentiment_pos','tweetos'], axis=1, inplace=True)\n",
    "    print( \"analyse finie\" )\n",
    "    return df_tweets\n",
    "df_tweets=tweet_analysis(df_tweets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gender prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prediction des genres\n",
      "reading names.csv\n",
      "prediction des genres\n",
      "prediction finie\n"
     ]
    }
   ],
   "source": [
    "def pred_genre(dataframe):\n",
    "    print(\"prediction des genres\")\n",
    "    print(\"reading names.csv\")\n",
    "    genders = pd.read_csv(\"./csv/names.csv\", sep = ',', header=0, encoding='latin-1')\n",
    "    sizes = np.zeros(len(genders))\n",
    "    names = pd.Series(\"\",range(len(genders)))\n",
    "    for i, row in genders.iterrows():\n",
    "        sizes[i] = len(row['Name'])\n",
    "        names[i] = row['Name'].lower()\n",
    "    genders['Length'] = sizes\n",
    "    genders['Name'] = names\n",
    "    genders.sort_values(['Length'],ascending=False,inplace=True)\n",
    "    \n",
    "    dico = {}\n",
    "    for i, row in genders.iterrows():\n",
    "        dico[row['Name']]=row['prob.gender']\n",
    "        \n",
    "    male_titles = ['m', 'mr']\n",
    "    female_titles = ['mrs', 'ms', 'miss', 'lady', 'mme', 'mlle']\n",
    "    \n",
    "    def pred_genre_2( username, dico, male_titles, female_titles):\n",
    "        for t in male_titles:\n",
    "            if( username.startswith(t+\".\") or username.startswith(t+\" \")):\n",
    "                return 'Male'\n",
    "        for t in female_titles:\n",
    "            if( username.startswith(t+\".\") or username.startswith(t+\" \")):\n",
    "                return 'Female'\n",
    "        l1 = username.split(\" \")\n",
    "        mots = []\n",
    "        for i in l1:\n",
    "            l2 = i.split(\"-\")\n",
    "            mots += l2\n",
    "        for mot in mots:\n",
    "            try:\n",
    "                genre = dico[mot]\n",
    "                if( genre != 'Unknown'):\n",
    "                    return genre\n",
    "            except KeyError:\n",
    "                pass\n",
    "        return 'Unknown'\n",
    "        \n",
    "    print(\"prediction des genres\")\n",
    "    genres_predits = pd.Series(len(dataframe))\n",
    "    for i, row in dataframe.iterrows() :\n",
    "         genres_predits[i] = pred_genre_2(row['name'].lower(), dico, male_titles, female_titles)\n",
    "    \n",
    "    dataframe['gender_predicted'] = genres_predits\n",
    "    print(\"prediction finie\")\n",
    "    return dataframe\n",
    "\n",
    "df_tweets = pred_genre(df_tweets)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## To SQLite database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def df_toSQLite(df_tweets):\n",
    "    conn = sqlite3.connect(\"C:\\\\Users\\DIARRA née CISSE Bal\\Desktop\\projet_inf\\projet_inf\\db.sqlite3\")\n",
    "    cur = conn.cursor()\n",
    "    df_tweets.to_sql(\"sentiments_analysis_tweets\", conn,if_exists='append', index= False)\n",
    "    cur.close()\n",
    "    conn.close()\n",
    "    return(df_tweets)\n",
    "df_tweets=df_toSQLite(df_tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
