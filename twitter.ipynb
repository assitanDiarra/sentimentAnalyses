{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Twitter: sentiments analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\DIARRA n√©e CISSE Bal\\Anaconda3\\lib\\site-packages\\nltk\\twitter\\__init__.py:20: UserWarning: The twython library has not been installed. Some functionality from the twitter package will not be available.\n",
      "  warnings.warn(\"The twython library has not been installed. \"\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import tweepy\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import twitter\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from nltk.sentiment.util import *\n",
    "from nltk import tokenize\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import datetime as dt\n",
    "import ast\n",
    "from datetime import datetime\n",
    "from datetime import timezone \n",
    "import sqlite3\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tweets scrapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "remaining_tweets: 10000\n",
      "found 100 tweets\n",
      "remaining_tweets: 9900\n",
      "found 100 tweets\n",
      "remaining_tweets: 9800\n",
      "found 100 tweets\n",
      "remaining_tweets: 9700\n",
      "found 100 tweets\n",
      "remaining_tweets: 9600\n",
      "found 100 tweets\n",
      "remaining_tweets: 9500\n",
      "found 100 tweets\n",
      "remaining_tweets: 9400\n",
      "found 100 tweets\n",
      "remaining_tweets: 9300\n",
      "found 100 tweets\n",
      "remaining_tweets: 9200\n",
      "found 100 tweets\n",
      "remaining_tweets: 9100\n",
      "found 100 tweets\n",
      "remaining_tweets: 9000\n",
      "found 100 tweets\n",
      "remaining_tweets: 8900\n",
      "found 100 tweets\n",
      "remaining_tweets: 8800\n",
      "found 100 tweets\n",
      "remaining_tweets: 8700\n",
      "found 100 tweets\n",
      "remaining_tweets: 8600\n",
      "found 100 tweets\n",
      "remaining_tweets: 8500\n",
      "found 100 tweets\n",
      "remaining_tweets: 8400\n",
      "found 100 tweets\n",
      "remaining_tweets: 8300\n",
      "found 100 tweets\n",
      "remaining_tweets: 8200\n",
      "found 100 tweets\n",
      "remaining_tweets: 8100\n",
      "found 100 tweets\n",
      "remaining_tweets: 8000\n",
      "found 100 tweets\n",
      "remaining_tweets: 7900\n",
      "found 100 tweets\n",
      "remaining_tweets: 7800\n",
      "found 100 tweets\n",
      "remaining_tweets: 7700\n",
      "found 100 tweets\n",
      "remaining_tweets: 7600\n",
      "found 100 tweets\n",
      "remaining_tweets: 7500\n",
      "found 100 tweets\n",
      "remaining_tweets: 7400\n",
      "found 100 tweets\n",
      "remaining_tweets: 7300\n",
      "found 100 tweets\n",
      "remaining_tweets: 7200\n",
      "found 100 tweets\n",
      "remaining_tweets: 7100\n",
      "found 100 tweets\n",
      "remaining_tweets: 7000\n",
      "found 100 tweets\n",
      "remaining_tweets: 6900\n",
      "found 100 tweets\n",
      "remaining_tweets: 6800\n",
      "found 100 tweets\n",
      "remaining_tweets: 6700\n",
      "found 100 tweets\n",
      "remaining_tweets: 6600\n",
      "found 100 tweets\n",
      "remaining_tweets: 6500\n",
      "found 100 tweets\n",
      "remaining_tweets: 6400\n",
      "found 100 tweets\n",
      "remaining_tweets: 6300\n",
      "found 100 tweets\n",
      "remaining_tweets: 6200\n",
      "found 100 tweets\n",
      "remaining_tweets: 6100\n",
      "found 100 tweets\n",
      "remaining_tweets: 6000\n",
      "found 100 tweets\n",
      "remaining_tweets: 5900\n",
      "found 100 tweets\n",
      "remaining_tweets: 5800\n",
      "found 100 tweets\n",
      "remaining_tweets: 5700\n",
      "found 100 tweets\n",
      "remaining_tweets: 5600\n",
      "found 100 tweets\n",
      "remaining_tweets: 5500\n",
      "found 100 tweets\n",
      "remaining_tweets: 5400\n",
      "found 100 tweets\n",
      "remaining_tweets: 5300\n",
      "found 100 tweets\n",
      "remaining_tweets: 5200\n",
      "found 100 tweets\n",
      "remaining_tweets: 5100\n",
      "found 100 tweets\n",
      "remaining_tweets: 5000\n",
      "found 100 tweets\n",
      "remaining_tweets: 4900\n",
      "found 100 tweets\n",
      "remaining_tweets: 4800\n",
      "found 100 tweets\n",
      "remaining_tweets: 4700\n",
      "found 100 tweets\n",
      "remaining_tweets: 4600\n",
      "found 100 tweets\n",
      "remaining_tweets: 4500\n",
      "found 100 tweets\n",
      "remaining_tweets: 4400\n",
      "found 100 tweets\n",
      "remaining_tweets: 4300\n",
      "found 100 tweets\n",
      "remaining_tweets: 4200\n",
      "found 100 tweets\n",
      "remaining_tweets: 4100\n",
      "found 100 tweets\n",
      "remaining_tweets: 4000\n",
      "found 100 tweets\n",
      "remaining_tweets: 3900\n",
      "found 100 tweets\n",
      "remaining_tweets: 3800\n",
      "found 100 tweets\n",
      "remaining_tweets: 3700\n",
      "found 100 tweets\n",
      "remaining_tweets: 3600\n",
      "found 100 tweets\n",
      "remaining_tweets: 3500\n",
      "found 100 tweets\n",
      "remaining_tweets: 3400\n",
      "found 100 tweets\n",
      "remaining_tweets: 3300\n",
      "found 100 tweets\n",
      "remaining_tweets: 3200\n",
      "found 100 tweets\n",
      "remaining_tweets: 3100\n",
      "found 100 tweets\n",
      "remaining_tweets: 3000\n",
      "found 100 tweets\n",
      "remaining_tweets: 2900\n",
      "found 100 tweets\n",
      "remaining_tweets: 2800\n",
      "found 100 tweets\n",
      "remaining_tweets: 2700\n",
      "found 100 tweets\n",
      "remaining_tweets: 2600\n",
      "found 100 tweets\n",
      "remaining_tweets: 2500\n",
      "found 100 tweets\n",
      "remaining_tweets: 2400\n",
      "found 100 tweets\n",
      "remaining_tweets: 2300\n",
      "found 100 tweets\n",
      "remaining_tweets: 2200\n",
      "found 100 tweets\n",
      "remaining_tweets: 2100\n",
      "found 100 tweets\n",
      "remaining_tweets: 2000\n",
      "found 100 tweets\n",
      "remaining_tweets: 1900\n",
      "found 100 tweets\n",
      "remaining_tweets: 1800\n",
      "found 100 tweets\n",
      "remaining_tweets: 1700\n",
      "found 100 tweets\n",
      "remaining_tweets: 1600\n",
      "found 100 tweets\n",
      "remaining_tweets: 1500\n",
      "found 100 tweets\n",
      "remaining_tweets: 1400\n",
      "found 100 tweets\n",
      "remaining_tweets: 1300\n",
      "found 100 tweets\n",
      "remaining_tweets: 1200\n",
      "found 100 tweets\n",
      "remaining_tweets: 1100\n",
      "found 100 tweets\n",
      "remaining_tweets: 1000\n",
      "found 100 tweets\n",
      "remaining_tweets: 900\n",
      "found 100 tweets\n",
      "remaining_tweets: 800\n",
      "found 100 tweets\n",
      "remaining_tweets: 700\n",
      "found 100 tweets\n",
      "remaining_tweets: 600\n",
      "found 100 tweets\n",
      "remaining_tweets: 500\n",
      "found 100 tweets\n",
      "remaining_tweets: 400\n",
      "found 100 tweets\n",
      "remaining_tweets: 300\n",
      "found 100 tweets\n",
      "remaining_tweets: 200\n",
      "found 100 tweets\n",
      "remaining_tweets: 100\n",
      "found 100 tweets\n"
     ]
    }
   ],
   "source": [
    "def tweet_search(query, max_tweets,langue):\n",
    "    ''' Function that takes in a search string 'query', the maximum\n",
    "        number of tweets 'max_tweets', and the minimum (i.e., starting)\n",
    "        tweet id. It returns a list of tweepy.models.Status objects. '''\n",
    "    \n",
    "    global liste\n",
    "    api = twitter.Api(consumer_key='GsZD2aeSrKjX1mhpyLwQ3sk6D',\n",
    "  consumer_secret='i6h6mcoKUQ4ReffNRnZWMO2cOvZJg61IugUj2kSjpGmalPdFG6',\n",
    "  access_token_key='969881872124841984-gOZs4DfzAWD2XlxqWsOIam9hestJWlH',\n",
    "  access_token_secret='eTBWSq7ju1Qap896DLQxZDZN2bBkpt1elmnKkCZ7KXnYJ')\n",
    " \n",
    "    searched_tweets = []\n",
    "    while len(searched_tweets) < max_tweets:\n",
    "        remaining_tweets = max_tweets - len(searched_tweets)\n",
    "        print(\"remaining_tweets:\",remaining_tweets)\n",
    "        try:\n",
    "            new_tweets = api.GetSearch(query, count=100,lang=langue)\n",
    "            print('found',len(new_tweets),'tweets')\n",
    "            for tweet in new_tweets:\n",
    "                liste.append({\"id\":tweet.id,\"name\":tweet.user.name,\"tweet\": tweet.text,\"created at\": tweet.created_at, \"place\":tweet.place})\n",
    "                #print(\"id:\",tweet.id,\"name:\",tweet.user.name,\"tweet:\",tweet.text,\"langue:\",tweet.lang,\"created at:\", tweet.created_at, \"location:\", tweet.location,\"place:\",tweet.place,\"user_name:\",tweet.user.location,\"user_location:\",tweet.user.location)\n",
    "            if not new_tweets:\n",
    "                print('no tweets found')\n",
    "                break\n",
    "            searched_tweets.extend(new_tweets)\n",
    "        except twitter.error.TwitterError: \n",
    "            print('exception raised, waiting 15 minutes')\n",
    "            print('(until:', dt.datetime.now()+dt.timedelta(minutes=15), ')')\n",
    "            time.sleep(15*60)\n",
    "            #break # stop the loop\n",
    "    return liste\n",
    "\n",
    "liste=[]\n",
    "a=tweet_search(query=\"e\",max_tweets=10000,langue=\"en\")\n",
    "df_tweets=pd.DataFrame(a)\n",
    "#import tempfile\n",
    "#tempfile.tempdir='C:/Temp/'\n",
    "#with tempfile.NamedTemporaryFile() as temp:\n",
    "\n",
    "#    df_tweets.to_csv(temp.name + '.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#tweets1=pd.read_csv(\"tweets1.csv\",encoding = \"ISO-8859-1\") \n",
    "#tweets2=pd.read_csv(\"tweets2.csv\",encoding = \"ISO-8859-1\",engine='python') \n",
    "#tweets3=pd.read_csv(\"tweets3.csv\",encoding = \"ISO-8859-1\",low_memory=False)\n",
    "#tweets4=pd.read_csv(\"tweets4.csv\",encoding = \"ISO-8859-1\",engine='python')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tweets cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\DIARRA n√©e CISSE Bal\\Anaconda3\\lib\\site-packages\\pandas\\core\\indexing.py:179: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  self._setitem_with_indexer(indexer, value)\n"
     ]
    }
   ],
   "source": [
    "def clean_tweets(liste_df):\n",
    "    df_tweets=pd.concat(liste_df)\n",
    "    # Drop tweets chere place is unknow\n",
    "    df_tweets=df_tweets.loc[df_tweets.place.notnull()]\n",
    "    # Drop duplicates\n",
    "    df_tweets.drop_duplicates(subset='tweet',inplace=True)\n",
    "    df_tweets.reset_index(drop=True, inplace=True)\n",
    "    dateTime=[]\n",
    "    df_tweets['id']=df_tweets['id'].astype('int64')\n",
    "    # column country and country_code\n",
    "    df_tweets['place']=df_tweets['place'].astype('str')\n",
    "    df_tweets=df_tweets.join(df_tweets['place'].apply(ast.literal_eval).apply(pd.Series)['country'])\n",
    "    df_tweets=df_tweets.join(df_tweets['place'].apply(ast.literal_eval).apply(pd.Series)['country_code'])\n",
    "    \n",
    "    # Date parsing\n",
    "    for x in df_tweets[\"created at\"]:\n",
    "        dateTime.append(datetime.strptime(x, '%a %b %d %H:%M:%S %z %Y').replace(\n",
    "            tzinfo=timezone.utc).astimezone(tz=None).strftime('%Y-%m-%d %H:%M:%S'))\n",
    "    df_tweets['datetime']=dateTime\n",
    "    # from country code of two letters to country code of tree letters\n",
    "    country_code= pd.read_csv(\"country_codes.csv\",encoding = \"ISO-8859-1\",sep=\";\",names=[\"Country\",\"country_code\",\"country_code3\"])  \n",
    "    for i in range(0,len(df_tweets)):\n",
    "        if any(country_code['country_code']==df_tweets['country_code'].loc[i]):\n",
    "            df_tweets['country_code'].loc[i]=country_code[country_code['country_code']==df_tweets['country_code'].loc[i]]['country_code3'].values[0]\n",
    "    # Dropping useless columns\n",
    "    if len(liste_df) ==1:\n",
    "        df_tweets.drop(['created at'], axis=1, inplace=True)\n",
    "\n",
    "    else:\n",
    "        df_tweets.drop(['Unnamed: 0','created at','location','user_name','user_location','langue'], axis=1, inplace=True)\n",
    "    return(df_tweets)\n",
    "\n",
    "df_tweets=clean_tweets([df_tweets])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentiments analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "analyse des sentiments\n",
      "analyse finie\n"
     ]
    }
   ],
   "source": [
    "def tweet_analysis(df_tweets):\n",
    "    \n",
    "    print( \"analyse des sentiments\" )\n",
    "    df_tweets['text_lem'] = [''.join([WordNetLemmatizer().lemmatize(re.sub('[^A-Za-z]', ' ', line)) \n",
    "                                  for line in lists]).strip() for lists in df_tweets['tweet']]       \n",
    "    sid = SentimentIntensityAnalyzer()\n",
    "    df_tweets['sentiment_compound_polarity']=df_tweets.text_lem.apply(lambda x:sid.polarity_scores(x)['compound'])\n",
    "    df_tweets['sentiment_neutral']=df_tweets.text_lem.apply(lambda x:sid.polarity_scores(x)['neu'])\n",
    "    df_tweets['sentiment_negative']=df_tweets.text_lem.apply(lambda x:sid.polarity_scores(x)['neg'])\n",
    "    df_tweets['sentiment_pos']=df_tweets.text_lem.apply(lambda x:sid.polarity_scores(x)['pos'])\n",
    "    df_tweets['sentiment_type']=''\n",
    "    df_tweets.loc[df_tweets.sentiment_compound_polarity>0,'sentiment_type']='POSITIVE'\n",
    "    df_tweets.loc[df_tweets.sentiment_compound_polarity==0,'sentiment_type']='NEUTRAL'\n",
    "    df_tweets.loc[df_tweets.sentiment_compound_polarity<0,'sentiment_type']='NEGATIVE'\n",
    "    df_tweets.drop(['sentiment_neutral','sentiment_negative','sentiment_pos',], axis=1, inplace=True)\n",
    "    print( \"analyse finie\" )\n",
    "    return df_tweets\n",
    "df_tweets=tweet_analysis(df_tweets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gender prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prediction des genres\n",
      "reading names.csv\n",
      "prediction des genres\n",
      "prediction finie\n"
     ]
    }
   ],
   "source": [
    "def pred_genre(dataframe):\n",
    "    print(\"prediction des genres\")\n",
    "    print(\"reading names.csv\")\n",
    "    genders = pd.read_csv(\"./names.csv\", sep = ',', header=0, encoding='latin-1')\n",
    "    sizes = np.zeros(len(genders))\n",
    "    names = pd.Series(\"\",range(len(genders)))\n",
    "    for i, row in genders.iterrows():\n",
    "        sizes[i] = len(row['Name'])\n",
    "        names[i] = row['Name'].lower()\n",
    "    genders['Length'] = sizes\n",
    "    genders['Name'] = names\n",
    "    genders.sort_values(['Length'],ascending=False,inplace=True)\n",
    "    \n",
    "    dico = {}\n",
    "    for i, row in genders.iterrows():\n",
    "        dico[row['Name']]=row['prob.gender']\n",
    "        \n",
    "    male_titles = ['m', 'mr']\n",
    "    female_titles = ['mrs', 'ms', 'miss', 'lady', 'mme', 'mlle']\n",
    "    \n",
    "    def pred_genre_2( username, dico, male_titles, female_titles):\n",
    "        for t in male_titles:\n",
    "            if( username.startswith(t+\".\") or username.startswith(t+\" \")):\n",
    "                return 'Male'\n",
    "        for t in female_titles:\n",
    "            if( username.startswith(t+\".\") or username.startswith(t+\" \")):\n",
    "                return 'Female'\n",
    "        l1 = username.split(\" \")\n",
    "        mots = []\n",
    "        for i in l1:\n",
    "            l2 = i.split(\"-\")\n",
    "            mots += l2\n",
    "        for mot in mots:\n",
    "            try:\n",
    "                genre = dico[mot]\n",
    "                if( genre != 'Unknown'):\n",
    "                    return genre\n",
    "            except KeyError:\n",
    "                pass\n",
    "        return 'Unknown'\n",
    "        \n",
    "    print(\"prediction des genres\")\n",
    "    genres_predits = pd.Series(len(dataframe))\n",
    "    for i, row in dataframe.iterrows() :\n",
    "         genres_predits[i] = pred_genre_2(row['name'].lower(), dico, male_titles, female_titles)\n",
    "    \n",
    "    dataframe['gender_predicted'] = genres_predits\n",
    "    print(\"prediction finie\")\n",
    "    return dataframe\n",
    "\n",
    "df_tweets = pred_genre(df_tweets)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## To SQLite database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def df_toSQLite(df_tweets):\n",
    "    conn = sqlite3.connect(\"projet inf\\projet_inf\\db.sqlite3\")\n",
    "    cur = conn.cursor()\n",
    "    df_tweets.to_sql(\"sentiments_analysis_tweets\", conn,if_exists='append', index= False)\n",
    "    cur.close()\n",
    "    conn.close()\n",
    "    return(df_tweets)\n",
    "df_tweets=df_toSQLite(df_tweets)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
